{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Object Streaming in AIStore\n",
    "\n",
    "The following demo shows how to use `ObjectFile` (`aistore.sdk.obj.object_file`) to stream large objects amidst potential instances of `ChunkedEncodingError` due to momentary issues with the cluster or its availability mid-read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5305d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Import Necessary Libraries\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "from aistore.sdk.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize AIStore Client with Retries\n",
    "\n",
    "AIS_ENDPOINT = \"http://localhost:8080\"\n",
    "\n",
    "# Define custom retry logic for requests to AIS. This will also be used when re-establishing streams (in the case of object.get().as_file()).\n",
    "# If you want to retry in the case of total pod failure, be sure to force retries on specific HTTP response codes that are not typically retried\n",
    "# In particular, 400 and 404 are what you might see as the client attempts to redirect requests to an object on a missing target\n",
    "# The timing on each retry is determined by (backoff_factor * 2^retry_count) -- here the last and longest retry waits 512 seconds\n",
    "retry = urllib3.Retry(total=10, backoff_factor=0.5, status_forcelist=[400, 404])\n",
    "client = Client(AIS_ENDPOINT, retry=retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5919a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare Bucket w/ ASR Tar File\n",
    "\n",
    "LIBRISPEECH_URL = \"http://www.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "DOWNLOADED_FILE_PATH = \"./dev-clean.tar.gz\"\n",
    "EXTRACT_PATH = \"./librispeech_extract\"\n",
    "OBJECT_NAME = \"librispeech-dev-clean.tar.gz\"\n",
    "BUCKET_NAME = \"test-librispeech-bucket\"\n",
    "\n",
    "# Step 2a: Download the compressed tar.gz file\n",
    "if not os.path.exists(DOWNLOADED_FILE_PATH):\n",
    "    response = requests.get(LIBRISPEECH_URL, stream=True, timeout=10)\n",
    "    with open(DOWNLOADED_FILE_PATH, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                print(f\"\\rDownloading {LIBRISPEECH_URL}... {f.tell() / 1024 / 1024:.2f}MB\", end=\"\")\n",
    "                f.write(chunk)\n",
    "\n",
    "# Step 2b: Upload the tar.gz file to AIStore directly\n",
    "client.bucket(BUCKET_NAME).create(exist_ok=True)\n",
    "client.bucket(BUCKET_NAME).object(OBJECT_NAME).put_file(DOWNLOADED_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6fc75",
   "metadata": {},
   "source": [
    "The `ObjectFile` implementation catches instances of `ChunkedEncodingError` mid-read and retries a new object stream from the last known position to resume safely, where `max_resume` dictates the number of resumes we will allow for a single read-operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b719d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Open the Object File & Read\n",
    "\n",
    "# Step 3a: Stream the object file and use tarfile.open to extract\n",
    "with client.bucket(BUCKET_NAME).object(OBJECT_NAME).get().as_file(max_resume=3) as file_obj:\n",
    "    with tarfile.open(fileobj=file_obj, mode='r|*') as tar:\n",
    "        if not os.path.exists(EXTRACT_PATH):\n",
    "            os.makedirs(EXTRACT_PATH)\n",
    "        tar.extractall(path=EXTRACT_PATH)\n",
    "\n",
    "# Step 3b: Print the directory structure\n",
    "def print_directory_tree(startpath, indent=\"|-- \"):\n",
    "    \"\"\"Prints the directory tree structure in a simple way.\"\"\"\n",
    "    for root, _, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, \"\").count(os.sep)\n",
    "        indent_str = \" \" * 4 * level + indent\n",
    "        print(f\"{indent_str}{os.path.basename(root)}/\")\n",
    "        subindent = \" \" * 4 * (level + 1) + indent\n",
    "        for f in files:\n",
    "            print(f\"{subindent}{f}\")\n",
    "\n",
    "print(\"Extracted Directory Structure:\")\n",
    "print_directory_tree(EXTRACT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 (Optional): Clean Up\n",
    "\n",
    "client.bucket(BUCKET_NAME).delete(missing_ok=True)\n",
    "os.remove(DOWNLOADED_FILE_PATH)\n",
    "if os.path.exists(EXTRACT_PATH):\n",
    "    os.system(f\"rm -rf {EXTRACT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec5602",
   "metadata": {},
   "source": [
    "For more information, please refer to the [Python SDK documentation](https://github.com/NVIDIA/aistore/blob/main/docs/python_sdk.md#object_file)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
